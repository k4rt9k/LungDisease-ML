# -*- coding: utf-8 -*-
"""Lung_Disease_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a_ooRJKgsjOgrTVutvTgNy7jDGAWFXTg
"""

# Step 1:Defining the Dataset and Unzipping the Dataset 
import zipfile
import os
zip_file_path = '/content/Data_set.zip'
unzip_dir = '/content/dataset'
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(unzip_dir)
print(f"Dataset successfully unzipped to: {unzip_dir}")
print("Contents of the unzipped directory:")
print(os.listdir(unzip_dir))


#Step 2: Explore the Dataset and Organize It for Training, Validation, and Test Sets
# Install the splitfolders library if not already installed
!pip install split-folders
import splitfolders
import os
# Path to the unzipped dataset folder
input_folder = '/content/dataset'  # Path where the dataset is unzipped
output_folder = '/content/split_dataset'  # Destination folder for the split datasets
# Split the dataset into training, validation, and test sets
splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(0.8, 0.1, 0.1))
# Verify the split folder structure
print("Dataset split into training, validation, and test sets:")
for root, dirs, files in os.walk(output_folder):
    print(f"Root: {root}")
    print(f"Directories: {dirs}")
    print(f"Files: {files}")
    print("-" * 50)


#Step 3: Preprocessing Images and Creating Data Loaders
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Paths for the training, validation, and test datasets
train_dir = '/content/split_dataset/train'
val_dir = '/content/split_dataset/val'
test_dir = '/content/split_dataset/test'
# Image size and batch size
IMG_SIZE = (224, 224)  # Resize images to 224x224 for the model
BATCH_SIZE = 32        # Number of images to process per batch
# Data augmentation for training images
train_datagen = ImageDataGenerator(
    rescale=1.0/255,         # Normalize pixel values to [0, 1]
    rotation_range=20,       # Random rotations
    width_shift_range=0.2,   # Random horizontal shifts
    height_shift_range=0.2,  # Random vertical shifts
    shear_range=0.2,         # Random shearing
    zoom_range=0.2,          # Random zoom
    horizontal_flip=True     # Random horizontal flips
)
# Data generators for validation and test sets (no augmentation, just normalization)
val_test_datagen = ImageDataGenerator(rescale=1.0/255)  # Normalize images to [0, 1]
# Load the training, validation, and test datasets using flow_from_directory
train_generator = train_datagen.flow_from_directory(
    train_dir,  # Path to the training data
    target_size=IMG_SIZE,  # Resize images
    batch_size=BATCH_SIZE,
    class_mode='categorical'  # Multi-class classification (3 classes)
)
val_generator = val_test_datagen.flow_from_directory(
    val_dir,  # Path to the validation data
    target_size=IMG_SIZE,  # Resize images
    batch_size=BATCH_SIZE,
    class_mode='categorical'  # Multi-class classification (3 classes)
)
test_generator = val_test_datagen.flow_from_directory(
    test_dir,  # Path to the test data
    target_size=IMG_SIZE,  # Resize images
    batch_size=BATCH_SIZE,
    class_mode='categorical'  # Multi-class classification (3 classes)
)
# Display the class labels (mapping of class indices to human-readable labels)
print("Class indices:", train_generator.class_indices)


#Step-4:Defining and Compiling the CNN Model
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16
# Load the pre-trained VGG16 model (excluding the top layer)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
# Freeze the base model to prevent retraining its weights
base_model.trainable = False
# Define the model architecture
def create_model():
    model = models.Sequential([
        base_model,  # Add the pre-trained VGG16 model
        layers.GlobalAveragePooling2D(),  # Global Average Pooling layer
        layers.Dense(512, activation='relu'),  # Fully connected layer with 512 units
        layers.Dropout(0.5),  # Dropout layer for regularization
        layers.Dense(3, activation='softmax')  # Output layer with 3 units (for 3 classes)
    ])
    # Compile the model
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model
# Create the model
model = create_model()
# Display the model summary
model.summary()


#Step-5:Training the Model
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
# Train the model
history = model.fit(
    train_generator,  # Training data
    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # Number of batches per epoch
    epochs=10,  # Set the number of epochs (you can adjust this as needed)
    validation_data=val_generator,  # Validation data
    validation_steps=val_generator.samples // val_generator.batch_size,  # Number of batches for validation
    callbacks=[early_stopping]  # EarlyStopping to prevent overfitting
)
model.save('/content/lung_disease_model.h5')


#Step-6:Save the model
from tensorflow.keras.models import save_model
model.save('lung_disease_detection_model.h5')


#Step-7:Test Loss and Test Accuracy   
# Evaluate the model on the test dataset
test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")


#Step-8:Plot Training and Validation Loss
import matplotlib.pyplot as plt
# Assuming you have already trained your model and saved the history
# Example: history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50)
# 1. Plot Training and Validation Loss
def plot_loss(history):
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training vs. Validation Loss')
    plt.legend()
    plt.show()
# 2. Plot Training and Validation Accuracy
def plot_accuracy(history):
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs. Validation Accuracy')
    plt.legend()
    plt.show()
# Call the functions to visualize results
plot_loss(history)
plot_accuracy(history)
# 3. Logic to check for overfitting
train_loss = history.history['loss'][-1]   # Final training loss
val_loss = history.history['val_loss'][-1]  # Final validation loss
train_acc = history.history['accuracy'][-1]  # Final training accuracy
val_acc = history.history['val_accuracy'][-1]  # Final validation accuracy
print("Training Loss: {:.4f}, Validation Loss: {:.4f}".format(train_loss, val_loss))
print("Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}".format(train_acc, val_acc))
if (val_loss > train_loss) and (train_acc > val_acc):
    print("Your model is overfitting. Consider regularization techniques or early stopping.")
else:
    print("Your model is not overfitting. Performance seems balanced.")


#Step-9:Code for Making Predictions on New Images
from tensorflow.keras.preprocessing import image
import numpy as np
def predict_image(image_path, model):
    img = image.load_img(image_path, target_size=(224, 224))
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)
    prediction = model.predict(img_array)
    class_labels = ['Lung Opacity', 'Normal', 'Viral Pneumonia']
    return class_labels[np.argmax(prediction)]
# Example usage
image_path ='/content/dataset/Viral Pneumonia/1001.jpg'
predicted_class = predict_image(image_path, model)
print(f"Predicted Class: {predicted_class}")


'''Step-10:
prompt: The requirements.txt file lists all the Python libraries your project needs.
When deploying to Hugging Face Spaces, this file ensures the correct libraries are installed.'''
!pip install split-folders matplotlib numpy tensorflow


#Step-11:Confusion matrix
import pickle
with open('/content/lung_disease_model.pkl', 'wb') as file:
    pickle.dump(model, file)
from sklearn.metrics import confusion_matrix
import seaborn as sns
# Predict on the test set
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)
# Get the true labels from the test generator
y_true = test_generator.classes
# Confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)
# Plot confusion matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()